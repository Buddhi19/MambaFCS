{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e5d9fa",
   "metadata": {},
   "source": [
    "# Annotations Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803cf9f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962c7d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddhiw/miniconda3/envs/Mamba/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:252: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:260: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:275: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:283: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:296: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:304: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "main_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(main_dir)\n",
    "print(main_dir)\n",
    "\n",
    "from MambaFCS.changedetection.configs.config import get_config\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as topt\n",
    "from tqdm import tqdm\n",
    "from MambaFCS.changedetection.datasets.make_data_loader import ChangeDetectionDatset, make_data_loader\n",
    "from MambaFCS.changedetection.utils_func.metrics import Evaluator\n",
    "from MambaFCS.changedetection.models.MambaBCD import STMambaBCD\n",
    "import MambaFCS.changedetection.utils_func.lovasz_loss as L\n",
    "from MambaFCS.changedetection.utils_func.loss import ce2_dice1\n",
    "import MambaFCS.changedetection.datasets.imutils as imutils\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "def getPath(env_path):\n",
    "    return os.path.expanduser(os.getenv(env_path))\n",
    "\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512ff4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_path = os.path.join(main_dir, 'MambaFCS/changedetection/configs/vssm1/vssm_base_224.yaml')\n",
    "\n",
    "model_path_SYSU = os.path.abspath('/storage/scratch3/buddhiw-change-detection/Mamba/MambaBCD_SYSU_base/12500_model_0.8151406739201706.pth')\n",
    "model_path_LEVIR = os.path.abspath('/storage/scratch3/buddhiw-change-detection/Mamba/MambaBCD_LEVIR_base/32500_model_0.9259150291513656.pth')\n",
    "model_path_WHU = os.path.abspath('/storage/scratch3/buddhiw-change-detection/Mamba/MambaBCD_WHU_base/20000_model_0.9389692423043274.pth')\n",
    "\n",
    "LEVIR_dataset_path = getPath('LEVIRCDPATH')\n",
    "\n",
    "LEVIR_train_dataset_path = os.path.join(LEVIR_dataset_path, 'train')\n",
    "LEVIR_test_dataset_path = os.path.join(LEVIR_dataset_path, 'test')\n",
    "LEVIR_train_data_list_path = os.path.join(LEVIR_dataset_path, 'train.txt')\n",
    "LEVIR_test_data_list_path = os.path.join(LEVIR_dataset_path, 'test.txt')\n",
    "\n",
    "SYSU_dataset_path = getPath('SYSUCDPATH')\n",
    "\n",
    "SYSU_train_dataset_path = os.path.join(SYSU_dataset_path, 'train')\n",
    "SYSU_test_dataset_path = os.path.join(SYSU_dataset_path, 'test')\n",
    "SYSU_train_data_list_path = os.path.join(SYSU_dataset_path, 'train.txt')\n",
    "SYSU_test_data_list_path = os.path.join(SYSU_dataset_path, 'test.txt')\n",
    "\n",
    "WHU_dataset_path = getPath('WHUCDPATH')\n",
    "\n",
    "WHU_train_dataset_path = os.path.join(WHU_dataset_path, 'train')\n",
    "WHU_test_dataset_path = os.path.join(WHU_dataset_path, 'test')\n",
    "WHU_train_data_list_path = os.path.join(WHU_dataset_path, 'train.txt')\n",
    "WHU_test_data_list_path = os.path.join(WHU_dataset_path, 'test.txt')\n",
    "\n",
    "VSSM_MODEL_PATH = getPath('VSSMBASEPATH')\n",
    "model_path = os.path.abspath('/storage/scratch3/buddhiw-change-detection/Mamba/')\n",
    "\n",
    "SAVE_IMGS_PATH = os.path.join(main_dir, 'MambaFCS/annotations/Images/BCD/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b2b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = []\n",
    "with open(LEVIR_train_data_list_path, 'r') as f:\n",
    "    for line in f:\n",
    "        train_data_list.append(line.strip())\n",
    "\n",
    "test_data_list = []\n",
    "with open(LEVIR_test_data_list_path, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data_list.append(line.strip())\n",
    "\n",
    "class ARGS:\n",
    "    def __init__(self):\n",
    "        self.dataset_path = LEVIR_dataset_path\n",
    "        self.pretrained_weight_path = VSSM_MODEL_PATH\n",
    "        self.dataset = 'LEVIR-CD+'\n",
    "        self.opts = None\n",
    "        self.type = 'train'\n",
    "        self.shuffle = True\n",
    "        self.crop_size = 256\n",
    "        self.batch_size = 4\n",
    "        self.max_iters = 1600000\n",
    "        self.start_iter = 0\n",
    "        self.cuda = True\n",
    "        self.model_type = 'MambaBCD'\n",
    "        self.learning_rate = 1e-4\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 5e-4\n",
    "        self.train_dataset_path = LEVIR_train_dataset_path\n",
    "        self.train_data_name_list = train_data_list\n",
    "        self.test_dataset_path = LEVIR_test_dataset_path\n",
    "        self.test_data_name_list = test_data_list\n",
    "        self.cfg = configs_path\n",
    "        self.model_param_path = model_path\n",
    "        self.resume = model_path_LEVIR\n",
    "\n",
    "        self.optimizer = 'adamw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9c0206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LEVIR_main():\n",
    "    args = ARGS()\n",
    "    return args\n",
    "\n",
    "def SYSU_main():\n",
    "    args = ARGS()\n",
    "    args.dataset = 'SYSU'\n",
    "    args.resume = model_path_SYSU\n",
    "    args.model_saving_name = 'MambaBCD_SYSU_base'\n",
    "    args.dataset_path = SYSU_dataset_path\n",
    "    args.train_dataset_path = SYSU_train_dataset_path\n",
    "    args.train_data_name_list = []\n",
    "    with open(SYSU_train_data_list_path, 'r') as f:\n",
    "        for line in f:\n",
    "            args.train_data_name_list.append(line.strip())\n",
    "    args.test_dataset_path = SYSU_test_dataset_path\n",
    "    args.test_data_name_list = []\n",
    "    with open(SYSU_test_data_list_path, 'r') as f:\n",
    "        for line in f:\n",
    "            args.test_data_name_list.append(line.strip())\n",
    "    \n",
    "    return args\n",
    "\n",
    "def WHU_main():\n",
    "    args = ARGS()\n",
    "    args.dataset = 'WHU-CD'\n",
    "    args.resume = model_path_WHU\n",
    "    args.model_saving_name = 'MambaBCD_WHU_base'\n",
    "    args.dataset_path = WHU_dataset_path\n",
    "    args.train_dataset_path = WHU_train_dataset_path\n",
    "    args.train_data_name_list = []\n",
    "    with open(WHU_train_data_list_path, 'r') as f:\n",
    "        for line in f:\n",
    "            args.train_data_name_list.append(line.strip())\n",
    "    args.test_dataset_path = WHU_test_dataset_path\n",
    "    args.test_data_name_list = []\n",
    "    with open(WHU_test_data_list_path, 'r') as f:\n",
    "        for line in f:\n",
    "            args.test_data_name_list.append(line.strip())\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5f48e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from /storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/changedetection/configs/vssm1/vssm_base_224.yaml\n",
      "Successfully load ckpt /storage/scratch3/buddhiw-change-detection/ChangeDetection/VSSModels/pretrained/vssm_base_0229_ckpt_epoch_237.pth\n",
      "_IncompatibleKeys(missing_keys=['outnorm0.weight', 'outnorm0.bias', 'outnorm1.weight', 'outnorm1.bias', 'outnorm2.weight', 'outnorm2.bias', 'outnorm3.weight', 'outnorm3.bias'], unexpected_keys=['classifier.norm.weight', 'classifier.norm.bias', 'classifier.head.weight', 'classifier.head.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddhiw/miniconda3/envs/Mamba/lib/python3.13/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "STMambaBCD(\n",
       "  (encoder): Backbone_VSSM(\n",
       "    (patch_embed): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): Permute()\n",
       "      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Permute()\n",
       "      (4): GELU(approximate='none')\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (6): Permute()\n",
       "      (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (blocks): Sequential(\n",
       "          (0): VSSBlock(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=128, out_features=256, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.0)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): VSSBlock(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=128, out_features=256, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.030000001192092896)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Permute()\n",
       "          (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): Permute()\n",
       "          (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (blocks): Sequential(\n",
       "          (0): VSSBlock(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.06000000238418579)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): VSSBlock(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.09000000357627869)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Permute()\n",
       "          (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): Permute()\n",
       "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (blocks): Sequential(\n",
       "          (0): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.12000000476837158)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.15000000596046448)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.18000000715255737)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.21000000834465027)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.24000000953674316)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.27000001072883606)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.30000001192092896)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.33000001311302185)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.36000001430511475)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.39000001549720764)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.42000001668930054)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.45000001788139343)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.48000001907348633)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.5099999904632568)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.5400000214576721)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Permute()\n",
       "          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): Permute()\n",
       "          (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (blocks): Sequential(\n",
       "          (0): VSSBlock(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "              (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.5700000524520874)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): VSSBlock(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "              (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.6000000238418579)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Identity()\n",
       "      )\n",
       "    )\n",
       "    (outnorm0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (outnorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (outnorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (outnorm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): ChangeDecoder(\n",
       "    (st_block_41): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_31): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_21): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_11): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (fuse_layer_1): FFT_Fusion(\n",
       "      (fft_pre): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (fft_post): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (reduce_conv): Conv2d(5120, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (reduce_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reduce_relu): ReLU(inplace=True)\n",
       "      (ch_gate): ChannelGate(\n",
       "        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sp_gate): SpatialGate(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse_layer_2): FFT_Fusion(\n",
       "      (fft_pre): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (fft_post): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (reduce_conv): Conv2d(2560, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (reduce_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reduce_relu): ReLU(inplace=True)\n",
       "      (ch_gate): ChannelGate(\n",
       "        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sp_gate): SpatialGate(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse_layer_3): FFT_Fusion(\n",
       "      (fft_pre): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (fft_post): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (reduce_conv): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (reduce_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reduce_relu): ReLU(inplace=True)\n",
       "      (ch_gate): ChannelGate(\n",
       "        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sp_gate): SpatialGate(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse_layer_4): FFT_Fusion(\n",
       "      (fft_pre): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (fft_post): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (reduce_conv): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (reduce_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reduce_relu): ReLU(inplace=True)\n",
       "      (ch_gate): ChannelGate(\n",
       "        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sp_gate): SpatialGate(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (down_sample_1): PyramidFusion(\n",
       "      (shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1024)\n",
       "        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=96, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=96, out_features=1536, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (down_sample_2): PyramidFusion(\n",
       "      (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=48, out_features=768, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (down_sample_3): PyramidFusion(\n",
       "      (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
       "        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=24, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=24, out_features=384, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_3): ResBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=32, out_features=512, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_2): ResBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_1): ResBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=8, out_features=128, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (main_clf): PyramidFusion(\n",
       "    (shortcut): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch1): Sequential(\n",
       "      (0): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (branch5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)\n",
       "      (1): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (channel_att): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=6, out_features=0, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=0, out_features=6, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (spatial_att): SpatialAttention(\n",
       "      (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (final_conv): Sequential(\n",
       "      (0): Conv2d(6, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = SYSU_main()\n",
    "config = get_config(args)\n",
    "\n",
    "train_data_loader = make_data_loader(args)\n",
    "\n",
    "evaluator = Evaluator(num_class=2)\n",
    "\n",
    "deep_model = STMambaBCD(\n",
    "    pretrained=args.pretrained_weight_path,\n",
    "    patch_size=config.MODEL.VSSM.PATCH_SIZE, \n",
    "    in_chans=config.MODEL.VSSM.IN_CHANS, \n",
    "    num_classes=config.MODEL.NUM_CLASSES, \n",
    "    depths=config.MODEL.VSSM.DEPTHS, \n",
    "    dims=config.MODEL.VSSM.EMBED_DIM, \n",
    "    # ===================\n",
    "    ssm_d_state=config.MODEL.VSSM.SSM_D_STATE,\n",
    "    ssm_ratio=config.MODEL.VSSM.SSM_RATIO,\n",
    "    ssm_rank_ratio=config.MODEL.VSSM.SSM_RANK_RATIO,\n",
    "    ssm_dt_rank=(\"auto\" if config.MODEL.VSSM.SSM_DT_RANK == \"auto\" else int(config.MODEL.VSSM.SSM_DT_RANK)),\n",
    "    ssm_act_layer=config.MODEL.VSSM.SSM_ACT_LAYER,\n",
    "    ssm_conv=config.MODEL.VSSM.SSM_CONV,\n",
    "    ssm_conv_bias=config.MODEL.VSSM.SSM_CONV_BIAS,\n",
    "    ssm_drop_rate=config.MODEL.VSSM.SSM_DROP_RATE,\n",
    "    ssm_init=config.MODEL.VSSM.SSM_INIT,\n",
    "    forward_type=config.MODEL.VSSM.SSM_FORWARDTYPE,\n",
    "    # ===================\n",
    "    mlp_ratio=config.MODEL.VSSM.MLP_RATIO,\n",
    "    mlp_act_layer=config.MODEL.VSSM.MLP_ACT_LAYER,\n",
    "    mlp_drop_rate=config.MODEL.VSSM.MLP_DROP_RATE,\n",
    "    # ===================\n",
    "    drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "    patch_norm=config.MODEL.VSSM.PATCH_NORM,\n",
    "    norm_layer=config.MODEL.VSSM.NORM_LAYER,\n",
    "    downsample_version=config.MODEL.VSSM.DOWNSAMPLE,\n",
    "    patchembed_version=config.MODEL.VSSM.PATCHEMBED,\n",
    "    gmlp=config.MODEL.VSSM.GMLP,\n",
    "    use_checkpoint=config.TRAIN.USE_CHECKPOINT,\n",
    "    ) \n",
    "\n",
    "checkpoint = torch.load(args.resume)\n",
    "model_dict = {}\n",
    "state_dict = deep_model.state_dict()\n",
    "for k, v in checkpoint.items():\n",
    "    if k in state_dict:\n",
    "        model_dict[k] = v\n",
    "state_dict.update(model_dict)\n",
    "deep_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "deep_model.cuda()\n",
    "deep_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bd5053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_loader(path):\n",
    "    img = np.array(imageio.imread(path), np.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e832f1",
   "metadata": {},
   "source": [
    "# Ground Truth Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6985839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image(image_number, dataset):\n",
    "\n",
    "    if dataset == 'LEVIR-CD+':\n",
    "        path = LEVIR_test_dataset_path\n",
    "    elif dataset == 'SYSU':\n",
    "        path = SYSU_test_dataset_path\n",
    "    elif dataset == 'WHU-CD':\n",
    "        path = WHU_test_dataset_path\n",
    "\n",
    "    pre_image_path = os.path.join(path,\"T1\",f\"{image_number}.png\")\n",
    "    post_image_path = os.path.join(path,\"T2\",f\"{image_number}.png\")\n",
    "    GT_image_path = os.path.join(path,\"GT\",f\"{image_number}.png\")\n",
    "\n",
    "    pre_image = img_loader(pre_image_path)\n",
    "    post_image = img_loader(post_image_path)\n",
    "    GT_image = img_loader(GT_image_path)\n",
    "\n",
    "    pre_img = imutils.normalize_img(pre_image)  # imagenet normalization\n",
    "    pre_img = np.transpose(pre_img, (2, 0, 1))\n",
    "\n",
    "    post_img = imutils.normalize_img(post_image)  # imagenet normalization\n",
    "    post_img = np.transpose(post_img, (2, 0, 1))\n",
    "\n",
    "    GT_img = GT_image/255\n",
    "\n",
    "    # save original images\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_pre.png\"), pre_image.astype(np.uint8))\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_post.png\"), post_image.astype(np.uint8))\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_gt.png\"), GT_image.astype(np.uint8))\n",
    "\n",
    "    pre_tensor = TF.to_tensor(imutils.normalize_img(pre_image)).unsqueeze(0).cuda()\n",
    "    post_tensor = TF.to_tensor(imutils.normalize_img(post_image)).unsqueeze(0).cuda()\n",
    "    label_tensor = torch.tensor(GT_image / 255.0).unsqueeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = deep_model(pre_tensor, post_tensor)\n",
    "        output = output.data.cpu().numpy()\n",
    "        output = np.argmax(output, axis=1)\n",
    "\n",
    "    H, W = output[0].shape\n",
    "    overlay = np.zeros((H, W, 3), dtype=np.uint8)   # start black\n",
    "\n",
    "    white_mask = (output[0] == 1)\n",
    "    overlay[white_mask] = [255, 255, 255] \n",
    "\n",
    "    # True Negatives: output==0, gt==0   RED\n",
    "    tn_mask = (output[0] == 0) & (GT_img == 1)\n",
    "    overlay[tn_mask] = [255, 0, 0]\n",
    "\n",
    "    # False Positives: output==1, gt==0  GREEN\n",
    "    fp_mask = (output[0] == 1) & (GT_img == 0)\n",
    "    overlay[fp_mask] = [0, 255, 0]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Option-1: save just the coloured mask\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_TN_FP.png\"), overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273753d2",
   "metadata": {},
   "source": [
    "# WHUCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79d16f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "annotate_image() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m      1\u001b[39m images = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwhucd_00001\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_01163\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_02380\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_03840\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_05017\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_06165\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwhucd_00003\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_01167\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_02384\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_03849\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_05022\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_06202\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwhucd_00825\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_02152\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwhucd_03515\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    102\u001b[39m ]\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mannotate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: annotate_image() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "images = [\n",
    "    \"whucd_00001\", \"whucd_01163\", \"whucd_02380\", \"whucd_03840\", \"whucd_05017\", \"whucd_06165\",\n",
    "    \"whucd_00003\", \"whucd_01167\", \"whucd_02384\", \"whucd_03849\", \"whucd_05022\", \"whucd_06202\",\n",
    "    \"whucd_00016\", \"whucd_01179\", \"whucd_02386\", \"whucd_03868\", \"whucd_05026\", \"whucd_06210\",\n",
    "    \"whucd_00023\", \"whucd_01195\", \"whucd_02416\", \"whucd_03890\", \"whucd_05030\", \"whucd_06213\",\n",
    "    \"whucd_00027\", \"whucd_01207\", \"whucd_02421\", \"whucd_03895\", \"whucd_05037\", \"whucd_06229\",\n",
    "    \"whucd_00031\", \"whucd_01210\", \"whucd_02422\", \"whucd_03909\", \"whucd_05043\", \"whucd_06239\",\n",
    "    \"whucd_00033\", \"whucd_01211\", \"whucd_02434\", \"whucd_03910\", \"whucd_05061\", \"whucd_06244\",\n",
    "    \"whucd_00055\", \"whucd_01212\", \"whucd_02435\", \"whucd_03917\", \"whucd_05078\", \"whucd_06247\",\n",
    "    \"whucd_00057\", \"whucd_01216\", \"whucd_02439\", \"whucd_03952\", \"whucd_05080\", \"whucd_06249\",\n",
    "    \"whucd_00059\", \"whucd_01224\", \"whucd_02440\", \"whucd_03968\", \"whucd_05081\", \"whucd_06250\",\n",
    "    \"whucd_00073\", \"whucd_01230\", \"whucd_02443\", \"whucd_03975\", \"whucd_05090\", \"whucd_06276\",\n",
    "    \"whucd_00087\", \"whucd_01241\", \"whucd_02446\", \"whucd_03980\", \"whucd_05092\", \"whucd_06284\",\n",
    "    \"whucd_00089\", \"whucd_01243\", \"whucd_02458\", \"whucd_03985\", \"whucd_05098\", \"whucd_06295\",\n",
    "    \"whucd_00091\", \"whucd_01245\", \"whucd_02459\", \"whucd_04004\", \"whucd_05104\", \"whucd_06318\",\n",
    "    \"whucd_00092\", \"whucd_01248\", \"whucd_02473\", \"whucd_04018\", \"whucd_05150\", \"whucd_06321\",\n",
    "    \"whucd_00094\", \"whucd_01253\", \"whucd_02478\", \"whucd_04030\", \"whucd_05154\", \"whucd_06326\",\n",
    "    \"whucd_00099\", \"whucd_01254\", \"whucd_02481\", \"whucd_04037\", \"whucd_05167\", \"whucd_06337\",\n",
    "    \"whucd_00129\", \"whucd_01258\", \"whucd_02489\", \"whucd_04038\", \"whucd_05171\", \"whucd_06339\",\n",
    "    \"whucd_00138\", \"whucd_01287\", \"whucd_02495\", \"whucd_04048\", \"whucd_05187\", \"whucd_06340\",\n",
    "    \"whucd_00144\", \"whucd_01309\", \"whucd_02496\", \"whucd_04057\", \"whucd_05202\", \"whucd_06347\",\n",
    "    \"whucd_00148\", \"whucd_01315\", \"whucd_02538\", \"whucd_04071\", \"whucd_05222\", \"whucd_06362\",\n",
    "    \"whucd_00156\", \"whucd_01320\", \"whucd_02543\", \"whucd_04080\", \"whucd_05233\", \"whucd_06365\",\n",
    "    \"whucd_00159\", \"whucd_01342\", \"whucd_02562\", \"whucd_04089\", \"whucd_05238\", \"whucd_06386\",\n",
    "    \"whucd_00170\", \"whucd_01386\", \"whucd_02565\", \"whucd_04090\", \"whucd_05251\", \"whucd_06395\",\n",
    "    \"whucd_00176\", \"whucd_01390\", \"whucd_02572\", \"whucd_04097\", \"whucd_05264\", \"whucd_06417\",\n",
    "    \"whucd_00178\", \"whucd_01395\", \"whucd_02578\", \"whucd_04107\", \"whucd_05269\", \"whucd_06454\",\n",
    "    \"whucd_00199\", \"whucd_01396\", \"whucd_02606\", \"whucd_04108\", \"whucd_05272\", \"whucd_06460\",\n",
    "    \"whucd_00209\", \"whucd_01420\", \"whucd_02608\", \"whucd_04118\", \"whucd_05278\", \"whucd_06466\",\n",
    "    \"whucd_00222\", \"whucd_01425\", \"whucd_02609\", \"whucd_04127\", \"whucd_05280\", \"whucd_06482\",\n",
    "    \"whucd_00225\", \"whucd_01442\", \"whucd_02621\", \"whucd_04132\", \"whucd_05284\", \"whucd_06486\",\n",
    "    \"whucd_00226\", \"whucd_01447\", \"whucd_02631\", \"whucd_04140\", \"whucd_05285\", \"whucd_06494\",\n",
    "    \"whucd_00230\", \"whucd_01472\", \"whucd_02641\", \"whucd_04142\", \"whucd_05292\", \"whucd_06496\",\n",
    "    \"whucd_00234\", \"whucd_01500\", \"whucd_02661\", \"whucd_04145\", \"whucd_05293\", \"whucd_06497\",\n",
    "    \"whucd_00240\", \"whucd_01510\", \"whucd_02666\", \"whucd_04146\", \"whucd_05312\", \"whucd_06507\",\n",
    "    \"whucd_00264\", \"whucd_01515\", \"whucd_02671\", \"whucd_04155\", \"whucd_05316\", \"whucd_06509\",\n",
    "    \"whucd_00272\", \"whucd_01518\", \"whucd_02679\", \"whucd_04160\", \"whucd_05319\", \"whucd_06511\",\n",
    "    \"whucd_00275\", \"whucd_01519\", \"whucd_02702\", \"whucd_04163\", \"whucd_05324\", \"whucd_06519\",\n",
    "    \"whucd_00277\", \"whucd_01542\", \"whucd_02734\", \"whucd_04184\", \"whucd_05326\", \"whucd_06522\",\n",
    "    \"whucd_00293\", \"whucd_01543\", \"whucd_02750\", \"whucd_04185\", \"whucd_05341\", \"whucd_06525\",\n",
    "    \"whucd_00311\", \"whucd_01564\", \"whucd_02757\", \"whucd_04191\", \"whucd_05359\", \"whucd_06540\",\n",
    "    \"whucd_00320\", \"whucd_01566\", \"whucd_02765\", \"whucd_04196\", \"whucd_05361\", \"whucd_06543\",\n",
    "    \"whucd_00326\", \"whucd_01567\", \"whucd_02766\", \"whucd_04198\", \"whucd_05378\", \"whucd_06549\",\n",
    "    \"whucd_00329\", \"whucd_01568\", \"whucd_02793\", \"whucd_04200\", \"whucd_05395\", \"whucd_06560\",\n",
    "    \"whucd_00337\", \"whucd_01570\", \"whucd_02797\", \"whucd_04209\", \"whucd_05398\", \"whucd_06584\",\n",
    "    \"whucd_00348\", \"whucd_01599\", \"whucd_02810\", \"whucd_04255\", \"whucd_05406\", \"whucd_06587\",\n",
    "    \"whucd_00349\", \"whucd_01606\", \"whucd_02842\", \"whucd_04265\", \"whucd_05408\", \"whucd_06594\",\n",
    "    \"whucd_00365\", \"whucd_01613\", \"whucd_02853\", \"whucd_04269\", \"whucd_05409\", \"whucd_06601\",\n",
    "    \"whucd_00368\", \"whucd_01615\", \"whucd_02891\", \"whucd_04273\", \"whucd_05438\", \"whucd_06607\",\n",
    "    \"whucd_00373\", \"whucd_01624\", \"whucd_02933\", \"whucd_04292\", \"whucd_05455\", \"whucd_06645\",\n",
    "    \"whucd_00385\", \"whucd_01634\", \"whucd_02976\", \"whucd_04333\", \"whucd_05465\", \"whucd_06651\",\n",
    "    \"whucd_00403\", \"whucd_01645\", \"whucd_03019\", \"whucd_04335\", \"whucd_05466\", \"whucd_06669\",\n",
    "    \"whucd_00416\", \"whucd_01647\", \"whucd_03031\", \"whucd_04349\", \"whucd_05471\", \"whucd_06670\",\n",
    "    \"whucd_00417\", \"whucd_01648\", \"whucd_03036\", \"whucd_04360\", \"whucd_05475\", \"whucd_06695\",\n",
    "    \"whucd_00419\", \"whucd_01682\", \"whucd_03037\", \"whucd_04367\", \"whucd_05482\", \"whucd_06712\",\n",
    "    \"whucd_00424\", \"whucd_01724\", \"whucd_03054\", \"whucd_04376\", \"whucd_05488\", \"whucd_06715\",\n",
    "    \"whucd_00446\", \"whucd_01736\", \"whucd_03061\", \"whucd_04406\", \"whucd_05493\", \"whucd_06720\",\n",
    "    \"whucd_00447\", \"whucd_01759\", \"whucd_03068\", \"whucd_04434\", \"whucd_05497\", \"whucd_06722\",\n",
    "    \"whucd_00460\", \"whucd_01781\", \"whucd_03085\", \"whucd_04436\", \"whucd_05503\", \"whucd_06736\",\n",
    "    \"whucd_00465\", \"whucd_01797\", \"whucd_03089\", \"whucd_04448\", \"whucd_05509\", \"whucd_06759\",\n",
    "    \"whucd_00471\", \"whucd_01800\", \"whucd_03105\", \"whucd_04459\", \"whucd_05546\", \"whucd_06777\",\n",
    "    \"whucd_00477\", \"whucd_01806\", \"whucd_03109\", \"whucd_04466\", \"whucd_05561\", \"whucd_06798\",\n",
    "    \"whucd_00480\", \"whucd_01810\", \"whucd_03122\", \"whucd_04470\", \"whucd_05569\", \"whucd_06823\",\n",
    "    \"whucd_00491\", \"whucd_01812\", \"whucd_03128\", \"whucd_04478\", \"whucd_05587\", \"whucd_06866\",\n",
    "    \"whucd_00506\", \"whucd_01814\", \"whucd_03169\", \"whucd_04482\", \"whucd_05588\", \"whucd_06879\",\n",
    "    \"whucd_00519\", \"whucd_01818\", \"whucd_03171\", \"whucd_04489\", \"whucd_05596\", \"whucd_06885\",\n",
    "    \"whucd_00522\", \"whucd_01855\", \"whucd_03188\", \"whucd_04491\", \"whucd_05599\", \"whucd_06907\",\n",
    "    \"whucd_00551\", \"whucd_01871\", \"whucd_03193\", \"whucd_04492\", \"whucd_05604\", \"whucd_06930\",\n",
    "    \"whucd_00559\", \"whucd_01878\", \"whucd_03202\", \"whucd_04497\", \"whucd_05609\", \"whucd_06938\",\n",
    "    \"whucd_00563\", \"whucd_01882\", \"whucd_03208\", \"whucd_04506\", \"whucd_05627\", \"whucd_06939\",\n",
    "    \"whucd_00579\", \"whucd_01883\", \"whucd_03210\", \"whucd_04527\", \"whucd_05628\", \"whucd_06952\",\n",
    "    \"whucd_00580\", \"whucd_01886\", \"whucd_03230\", \"whucd_04538\", \"whucd_05632\", \"whucd_06966\",\n",
    "    \"whucd_00582\", \"whucd_01891\", \"whucd_03231\", \"whucd_04539\", \"whucd_05634\", \"whucd_06970\",\n",
    "    \"whucd_00595\", \"whucd_01897\", \"whucd_03244\", \"whucd_04546\", \"whucd_05636\", \"whucd_06975\",\n",
    "    \"whucd_00600\", \"whucd_01900\", \"whucd_03250\", \"whucd_04548\", \"whucd_05640\", \"whucd_06998\",\n",
    "    \"whucd_00612\", \"whucd_01905\", \"whucd_03255\", \"whucd_04558\", \"whucd_05643\", \"whucd_07008\",\n",
    "    \"whucd_00648\", \"whucd_01919\", \"whucd_03257\", \"whucd_04613\", \"whucd_05646\", \"whucd_07009\",\n",
    "    \"whucd_00658\", \"whucd_01920\", \"whucd_03295\", \"whucd_04620\", \"whucd_05647\", \"whucd_07016\",\n",
    "    \"whucd_00660\", \"whucd_01952\", \"whucd_03298\", \"whucd_04624\", \"whucd_05659\", \"whucd_07020\",\n",
    "    \"whucd_00662\", \"whucd_01971\", \"whucd_03299\", \"whucd_04634\", \"whucd_05667\", \"whucd_07034\",\n",
    "    \"whucd_00668\", \"whucd_01973\", \"whucd_03302\", \"whucd_04637\", \"whucd_05668\", \"whucd_07051\",\n",
    "    \"whucd_00671\", \"whucd_01998\", \"whucd_03322\", \"whucd_04652\", \"whucd_05713\", \"whucd_07073\",\n",
    "    \"whucd_00684\", \"whucd_02001\", \"whucd_03353\", \"whucd_04684\", \"whucd_05727\", \"whucd_07077\",\n",
    "    \"whucd_00685\", \"whucd_02003\", \"whucd_03357\", \"whucd_04687\", \"whucd_05729\", \"whucd_07079\",\n",
    "    \"whucd_00686\", \"whucd_02005\", \"whucd_03380\", \"whucd_04709\", \"whucd_05742\", \"whucd_07081\",\n",
    "    \"whucd_00690\", \"whucd_02014\", \"whucd_03388\", \"whucd_04718\", \"whucd_05754\", \"whucd_07089\",\n",
    "    \"whucd_00696\", \"whucd_02016\", \"whucd_03400\", \"whucd_04719\", \"whucd_05764\", \"whucd_07093\",\n",
    "    \"whucd_00698\", \"whucd_02024\", \"whucd_03401\", \"whucd_04733\", \"whucd_05772\", \"whucd_07102\",\n",
    "    \"whucd_00701\", \"whucd_02046\", \"whucd_03404\", \"whucd_04744\", \"whucd_05777\", \"whucd_07119\",\n",
    "    \"whucd_00711\", \"whucd_02047\", \"whucd_03412\", \"whucd_04765\", \"whucd_05797\", \"whucd_07121\",\n",
    "    \"whucd_00731\", \"whucd_02053\", \"whucd_03418\", \"whucd_04766\", \"whucd_05824\", \"whucd_07132\",\n",
    "    \"whucd_00735\", \"whucd_02056\", \"whucd_03429\", \"whucd_04773\", \"whucd_05825\", \"whucd_07141\",\n",
    "    \"whucd_00756\", \"whucd_02060\", \"whucd_03435\", \"whucd_04780\", \"whucd_05847\", \"whucd_07151\",\n",
    "    \"whucd_00758\", \"whucd_02064\", \"whucd_03437\", \"whucd_04785\", \"whucd_05858\", \"whucd_07164\",\n",
    "    \"whucd_00776\", \"whucd_02065\", \"whucd_03457\", \"whucd_04787\", \"whucd_05883\", \"whucd_07165\",\n",
    "    \"whucd_00777\", \"whucd_02077\", \"whucd_03458\", \"whucd_04797\", \"whucd_05893\", \"whucd_07168\",\n",
    "    \"whucd_00786\", \"whucd_02086\", \"whucd_03477\", \"whucd_04805\", \"whucd_05901\", \"whucd_07171\",\n",
    "    \"whucd_00810\", \"whucd_02109\", \"whucd_03481\", \"whucd_04810\", \"whucd_05927\", \"whucd_07189\",\n",
    "    \"whucd_00811\", \"whucd_02117\", \"whucd_03510\", \"whucd_04824\", \"whucd_05948\", \"whucd_07190\",\n",
    "    \"whucd_00814\", \"whucd_02146\", \"whucd_03515\", \"whucd_04825\", \"whucd_05951\", \"whucd_07197\",\n",
    "    \"whucd_00825\", \"whucd_02152\", \"whucd_03515\"\n",
    "]\n",
    "\n",
    "\n",
    "for img in images:\n",
    "    annotate_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e92142",
   "metadata": {},
   "source": [
    "# LEVIRCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7c6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_209599/3105635309.py:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = np.array(imageio.imread(path), np.float32)\n"
     ]
    }
   ],
   "source": [
    "Images = [\n",
    "    \"00638\", \"00639\", \"00640\", \"00641\", \"00642\", \"00643\", \"00644\", \"00645\", \"00646\", \"00647\", \"00648\", \"00649\", \"00650\", \"00651\", \"00652\", \"00653\",\n",
    "    \"00654\", \"00655\", \"00656\", \"00657\", \"00658\", \"00659\", \"00660\", \"00661\", \"00662\", \"00663\", \"00664\", \"00665\", \"00666\", \"00667\", \"00668\", \"00669\",\n",
    "    \"00670\", \"00671\", \"00672\", \"00673\", \"00674\", \"00675\", \"00676\", \"00677\", \"00678\", \"00679\", \"00680\", \"00681\", \"00682\", \"00683\", \"00684\", \"00685\",\n",
    "    \"00686\", \"00687\", \"00688\", \"00689\", \"00690\", \"00691\", \"00692\", \"00693\", \"00694\", \"00695\", \"00696\", \"00697\", \"00698\", \"00699\", \"00700\", \"00701\",\n",
    "    \"00702\", \"00703\", \"00704\", \"00705\", \"00706\", \"00707\", \"00708\", \"00709\", \"00710\", \"00711\", \"00712\", \"00713\", \"00714\", \"00715\", \"00716\", \"00717\",\n",
    "    \"00718\", \"00719\", \"00720\", \"00721\", \"00722\", \"00723\", \"00724\", \"00725\", \"00726\", \"00727\", \"00728\", \"00729\", \"00730\", \"00731\", \"00732\", \"00733\",\n",
    "    \"00734\", \"00735\", \"00736\", \"00737\", \"00738\", \"00739\", \"00740\", \"00741\", \"00742\", \"00743\", \"00744\", \"00745\", \"00746\", \"00747\", \"00748\", \"00749\",\n",
    "    \"00750\", \"00751\", \"00752\", \"00753\", \"00754\", \"00755\", \"00756\", \"00757\", \"00758\", \"00759\", \"00760\", \"00761\", \"00762\", \"00763\", \"00764\", \"00765\",\n",
    "    \"00766\", \"00767\", \"00768\", \"00769\", \"00770\", \"00771\", \"00772\", \"00773\", \"00774\", \"00775\", \"00776\", \"00777\", \"00778\", \"00779\", \"00780\", \"00781\",\n",
    "    \"00782\", \"00783\", \"00784\", \"00785\", \"00786\", \"00787\", \"00788\", \"00789\", \"00790\", \"00791\", \"00792\", \"00793\", \"00794\", \"00795\", \"00796\", \"00797\",\n",
    "    \"00798\", \"00799\", \"00800\", \"00801\", \"00802\", \"00803\", \"00804\", \"00805\", \"00806\", \"00807\", \"00808\", \"00809\", \"00810\", \"00811\", \"00812\", \"00813\",\n",
    "    \"00814\", \"00815\", \"00816\", \"00817\", \"00818\", \"00819\", \"00820\", \"00821\", \"00822\", \"00823\", \"00824\", \"00825\", \"00826\", \"00827\", \"00828\", \"00829\",\n",
    "    \"00830\", \"00831\", \"00832\", \"00833\", \"00834\", \"00835\", \"00836\", \"00837\", \"00838\", \"00839\", \"00840\", \"00841\", \"00842\", \"00843\", \"00844\", \"00845\",\n",
    "    \"00846\", \"00847\", \"00848\", \"00849\", \"00850\", \"00851\", \"00852\", \"00853\", \"00854\", \"00855\", \"00856\", \"00857\", \"00858\", \"00859\", \"00860\", \"00861\",\n",
    "    \"00862\", \"00863\", \"00864\", \"00865\", \"00866\", \"00867\", \"00868\", \"00869\", \"00870\", \"00871\", \"00872\", \"00873\", \"00874\", \"00875\", \"00876\", \"00877\",\n",
    "    \"00878\", \"00879\", \"00880\", \"00881\", \"00882\", \"00883\", \"00884\", \"00885\", \"00886\", \"00887\", \"00888\", \"00889\", \"00890\", \"00891\", \"00892\", \"00893\",\n",
    "    \"00894\", \"00895\", \"00896\", \"00897\", \"00898\", \"00899\", \"00900\", \"00901\", \"00902\", \"00903\", \"00904\", \"00905\", \"00906\", \"00907\", \"00908\", \"00909\",\n",
    "    \"00910\", \"00911\", \"00912\", \"00913\", \"00914\", \"00915\", \"00916\", \"00917\", \"00918\", \"00919\", \"00920\", \"00921\", \"00922\", \"00923\", \"00924\", \"00925\",\n",
    "    \"00926\", \"00927\", \"00928\", \"00929\", \"00930\", \"00931\", \"00932\", \"00933\", \"00934\", \"00935\", \"00936\", \"00937\", \"00938\", \"00939\", \"00940\", \"00941\",\n",
    "    \"00942\", \"00943\", \"00944\", \"00945\", \"00946\", \"00947\", \"00948\", \"00949\", \"00950\", \"00951\", \"00952\", \"00953\", \"00954\", \"00955\", \"00956\", \"00957\",\n",
    "    \"00958\", \"00959\", \"00960\", \"00961\", \"00962\", \"00963\", \"00964\", \"00965\", \"00966\", \"00967\", \"00968\", \"00969\", \"00970\", \"00971\", \"00972\", \"00973\",\n",
    "    \"00974\", \"00975\", \"00976\", \"00977\", \"00978\", \"00979\", \"00980\", \"00981\", \"00982\", \"00983\", \"00984\", \"00985\"\n",
    "]\n",
    "\n",
    "for img in Images:\n",
    "    annotate_image(img, \"LEVIR-CD+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816962f",
   "metadata": {},
   "source": [
    "# SYSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd1690e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_209599/3105635309.py:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = np.array(imageio.imread(path), np.float32)\n"
     ]
    }
   ],
   "source": [
    "Images = [\n",
    "     \"00638\", \"00639\", \"00640\", \"00641\", \"00642\", \"00643\", \"00644\", \"00645\", \"00646\", \"00647\",\n",
    "    \"00648\", \"00649\", \"00650\", \"00651\", \"00652\", \"00653\", \"00654\", \"00655\", \"00656\", \"00657\",\n",
    "    \"00658\", \"00659\", \"00660\", \"00661\", \"00662\", \"00663\", \"00664\", \"00665\", \"00666\", \"00667\",\n",
    "    \"00668\", \"00669\", \"00670\", \"00671\", \"00672\", \"00673\", \"00674\", \"00675\", \"00676\", \"00677\",\n",
    "    \"00678\", \"00679\", \"00680\", \"00681\", \"00682\", \"00683\", \"00684\", \"00685\", \"00686\", \"00687\",\n",
    "    \"00688\", \"00689\", \"00690\", \"00691\", \"00692\", \"00693\", \"00694\", \"00695\", \"00696\", \"00697\",\n",
    "    \"00698\", \"00699\", \"00700\", \"00701\", \"00702\", \"00703\", \"00704\", \"00705\", \"00706\", \"00707\",\n",
    "    \"00708\", \"00709\", \"00710\", \"00711\", \"00712\", \"00713\", \"00714\", \"00715\", \"00716\", \"00717\",\n",
    "    \"00718\", \"00719\", \"00720\", \"00721\", \"00722\", \"00723\", \"00724\", \"00725\", \"00726\", \"00727\",\n",
    "    \"00728\", \"00729\", \"00730\", \"00731\", \"00732\", \"00733\", \"00734\", \"00735\", \"00736\", \"00737\",\n",
    "    \"00738\", \"00739\", \"00740\", \"00741\", \"00742\", \"00743\", \"00744\", \"00745\", \"00746\", \"00747\",\n",
    "    \"00748\", \"00749\", \"00750\", \"00751\", \"00752\", \"00753\", \"00754\", \"00755\", \"00756\", \"00757\",\n",
    "    \"00758\", \"00759\", \"00760\", \"00761\", \"00762\", \"00763\", \"00764\", \"00765\", \"00766\", \"00767\",\n",
    "    \"00768\", \"00769\", \"00770\", \"00771\", \"00772\", \"00773\", \"00774\", \"00775\", \"00776\", \"00777\",\n",
    "    \"00778\", \"00779\", \"00780\", \"00781\", \"00782\", \"00783\", \"00784\", \"00785\", \"00786\", \"00787\",\n",
    "    \"00788\", \"00789\", \"00790\", \"00791\", \"00792\", \"00793\", \"00794\", \"00795\", \"00796\", \"00797\",\n",
    "    \"00798\", \"00799\", \"00800\", \"00801\", \"00802\", \"00803\", \"00804\", \"00805\", \"00806\", \"00807\",\n",
    "    \"00808\", \"00809\", \"00810\", \"00811\", \"00812\", \"00813\", \"00814\", \"00815\", \"00816\", \"00817\",\n",
    "    \"00818\", \"00819\", \"00820\", \"00821\", \"00822\", \"00823\", \"00824\", \"00825\", \"00826\", \"00827\",\n",
    "    \"00828\", \"00829\", \"00830\", \"00831\", \"00832\", \"00833\", \"00834\", \"00835\", \"00836\", \"00837\",\n",
    "    \"00838\", \"00839\", \"00840\", \"00841\", \"00842\", \"00843\", \"00844\", \"00845\", \"00846\", \"00847\",\n",
    "    \"00848\", \"00849\", \"00850\", \"00851\", \"00852\", \"00853\", \"00854\", \"00855\", \"00856\", \"00857\",\n",
    "    \"00858\", \"00859\", \"00860\", \"00861\", \"00862\", \"00863\", \"00864\", \"00865\", \"00866\", \"00867\",\n",
    "    \"00868\", \"00869\", \"00870\", \"00871\", \"00872\", \"00873\", \"00874\", \"00875\", \"00876\", \"00877\",\n",
    "    \"00878\", \"00879\", \"00880\", \"00881\", \"00882\", \"00883\", \"00884\", \"00885\", \"00886\", \"00887\",\n",
    "    \"00888\", \"00889\", \"00890\", \"00891\", \"00892\", \"00893\", \"00894\", \"00895\", \"00896\", \"00897\",\n",
    "    \"00898\", \"00899\", \"00900\", \"00901\", \"00902\", \"00903\", \"00904\", \"00905\", \"00906\", \"00907\",\n",
    "    \"00908\", \"00909\", \"00910\", \"00911\", \"00912\", \"00913\", \"00914\", \"00915\", \"00916\", \"00917\",\n",
    "    \"00918\", \"00919\", \"00920\", \"00921\", \"00922\", \"00923\", \"00924\", \"00925\", \"00926\", \"00927\",\n",
    "    \"00928\", \"00929\", \"00930\", \"00931\", \"00932\", \"00933\", \"00934\", \"00935\", \"00936\", \"00937\",\n",
    "    \"00938\", \"00939\", \"00940\", \"00941\", \"00942\", \"00943\", \"00944\", \"00945\", \"00946\", \"00947\"\n",
    "]\n",
    "\n",
    "for img in Images:\n",
    "    annotate_image(img, \"SYSU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
