{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e5d9fa",
   "metadata": {},
   "source": [
    "# Annotations Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803cf9f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c7d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddhiw/miniconda3/envs/Mamba/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:252: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:260: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:275: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:283: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:296: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/classification/models/vmamba.py:304: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "main_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(main_dir)\n",
    "print(main_dir)\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from MambaFCS.changedetection.datasets.make_data_loader import SemanticChangeDetectionDatset, make_data_loader, SemanticChangeDetectionDatset_LandSat\n",
    "from MambaFCS.changedetection.utils_func.metrics import Evaluator\n",
    "from MambaFCS.changedetection.models.STMambaSCD import STMambaSCD\n",
    "import MambaFCS.changedetection.utils_func.lovasz_loss as L\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from MambaFCS.changedetection.utils_func.mcd_utils import accuracy, SCDD_eval_all, AverageMeter\n",
    "from MambaFCS.changedetection.utils_func.loss import contrastive_loss, ce2_dice1, ce2_dice1_multiclass, SEK_loss_from_eval\n",
    "from MambaFCS.changedetection.configs.config import get_config\n",
    "import MambaFCS.changedetection.datasets.imutils as imutils\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "def getPath(env_path):\n",
    "    return os.path.expanduser(os.getenv(env_path))\n",
    "\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e10e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch3/buddhiw-change-detection/Datasets/Landsat-SCD/\n"
     ]
    }
   ],
   "source": [
    "VSSM_MODEL_PATH = getPath('VSSMBASEPATH')\n",
    "\n",
    "LandSat_DATASET_PATH = getPath('LANDSAT')\n",
    "print(LandSat_DATASET_PATH)\n",
    "LandSat_TRAIN_DATA_LIST_PATH = os.path.join(LandSat_DATASET_PATH, 'train_list.txt')\n",
    "\n",
    "LandSat_TEST_DATA_LIST_PATH = os.path.join(LandSat_DATASET_PATH, 'test_list.txt')\n",
    "\n",
    "LandSat_VAL_DATA_LIST_PATH = os.path.join(LandSat_DATASET_PATH, 'val_list.txt')\n",
    "\n",
    "configs_path = os.path.join(main_dir, 'MambaFCS/changedetection/configs/vssm1/vssm_base_224.yaml')\n",
    "\n",
    "model_path = os.path.abspath('/storage/scratch3/buddhiw-change-detection/Mamba/JOURNAL/')\n",
    "\n",
    "MAMBAFCS = os.path.abspath('/storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/saved_models/LandSat_SCD_with_DICE/75000_model_0.603.pth')\n",
    "\n",
    "SAVE_IMGS_PATH = os.path.join(main_dir, 'MambaFCS/annotations/Images/Landsat/')\n",
    "\n",
    "train_data_list = []\n",
    "with open(LandSat_TRAIN_DATA_LIST_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        train_data_list.append(line.strip())\n",
    "\n",
    "with open(LandSat_VAL_DATA_LIST_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        train_data_list.append(line.strip())\n",
    "\n",
    "\n",
    "test_data_list = []\n",
    "with open(LandSat_TEST_DATA_LIST_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data_list.append(line.strip())\n",
    "\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9c8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self):\n",
    "        self.cfg = configs_path\n",
    "        self.opts = None\n",
    "        self.pretrained_weight_path = VSSM_MODEL_PATH\n",
    "        self.dataset = 'Landsat'\n",
    "        self.type = 'train'\n",
    "        self.train_dataset_path = LandSat_TRAIN_DATA_LIST_PATH\n",
    "\n",
    "        self.test_dataset_path = LandSat_TEST_DATA_LIST_PATH\n",
    "\n",
    "        \n",
    "        self.shuffle = True\n",
    "        self.batch_size = 4\n",
    "        self.crop_size = 512\n",
    "        self.train_data_name_list = train_data_list\n",
    "        self.test_data_name_list = test_data_list\n",
    "        self.start_iter = 0\n",
    "        self.cuda = True\n",
    "        self.max_iters = 800000\n",
    "        self.model_type = 'MambaSCD_base'\n",
    "        self.model_param_path = model_path\n",
    "\n",
    "        self.resume = MAMBAFCS\n",
    "\n",
    "        self.learning_rate = 1e-4\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 5e-4\n",
    "        self.num_classes = 5\n",
    "\n",
    "args = ARGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12cef77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from /storage/scratch3/buddhiw-change-detection/ChangeDetection/CDMamba/MambaFCS/changedetection/configs/vssm1/vssm_base_224.yaml\n",
      "Successfully load ckpt /storage/scratch3/buddhiw-change-detection/ChangeDetection/VSSModels/pretrained/vssm_base_0229_ckpt_epoch_237.pth\n",
      "_IncompatibleKeys(missing_keys=['outnorm0.weight', 'outnorm0.bias', 'outnorm1.weight', 'outnorm1.bias', 'outnorm2.weight', 'outnorm2.bias', 'outnorm3.weight', 'outnorm3.bias'], unexpected_keys=['classifier.norm.weight', 'classifier.norm.bias', 'classifier.head.weight', 'classifier.head.bias'])\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddhiw/miniconda3/envs/Mamba/lib/python3.13/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacity of 47.52 GiB of which 4.38 MiB is free. Process 2911810 has 15.66 GiB memory in use. Process 2764986 has 15.55 GiB memory in use. Process 444288 has 7.17 GiB memory in use. Process 2816589 has 7.17 GiB memory in use. Process 930308 has 1.56 GiB memory in use. Including non-PyTorch memory, this process has 358.00 MiB memory in use. Of the allocated memory 95.51 MiB is allocated by PyTorch, and 2.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m      1\u001b[39m config = get_config(args)\n\u001b[32m      3\u001b[39m deep_model = STMambaSCD(\n\u001b[32m      4\u001b[39m     output_cd = \u001b[32m2\u001b[39m, \n\u001b[32m      5\u001b[39m     output_clf = args.num_classes,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     use_checkpoint=config.TRAIN.USE_CHECKPOINT,\n\u001b[32m     35\u001b[39m     ) \n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m deep_model = \u001b[43mdeep_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Mamba/lib/python3.13/site-packages/torch/nn/modules/module.py:1053\u001b[39m, in \u001b[36mModule.cuda\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n\u001b[32m   1037\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[32m   1038\u001b[39m \n\u001b[32m   1039\u001b[39m \u001b[33;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1051\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Mamba/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Mamba/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 903 (4 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Mamba/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Mamba/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Mamba/lib/python3.13/site-packages/torch/nn/modules/module.py:1053\u001b[39m, in \u001b[36mModule.cuda.<locals>.<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n\u001b[32m   1037\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[32m   1038\u001b[39m \n\u001b[32m   1039\u001b[39m \u001b[33;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1051\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacity of 47.52 GiB of which 4.38 MiB is free. Process 2911810 has 15.66 GiB memory in use. Process 2764986 has 15.55 GiB memory in use. Process 444288 has 7.17 GiB memory in use. Process 2816589 has 7.17 GiB memory in use. Process 930308 has 1.56 GiB memory in use. Including non-PyTorch memory, this process has 358.00 MiB memory in use. Of the allocated memory 95.51 MiB is allocated by PyTorch, and 2.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "config = get_config(args)\n",
    "\n",
    "deep_model = STMambaSCD(\n",
    "    output_cd = 2, \n",
    "    output_clf = args.num_classes,\n",
    "    pretrained=args.pretrained_weight_path,\n",
    "    patch_size=config.MODEL.VSSM.PATCH_SIZE, \n",
    "    in_chans=config.MODEL.VSSM.IN_CHANS, \n",
    "    num_classes=config.MODEL.NUM_CLASSES, \n",
    "    depths=config.MODEL.VSSM.DEPTHS, \n",
    "    dims=config.MODEL.VSSM.EMBED_DIM, \n",
    "    # ===================\n",
    "    ssm_d_state=config.MODEL.VSSM.SSM_D_STATE,\n",
    "    ssm_ratio=config.MODEL.VSSM.SSM_RATIO,\n",
    "    ssm_rank_ratio=config.MODEL.VSSM.SSM_RANK_RATIO,\n",
    "    ssm_dt_rank=(\"auto\" if config.MODEL.VSSM.SSM_DT_RANK == \"auto\" else int(config.MODEL.VSSM.SSM_DT_RANK)),\n",
    "    ssm_act_layer=config.MODEL.VSSM.SSM_ACT_LAYER,\n",
    "    ssm_conv=config.MODEL.VSSM.SSM_CONV,\n",
    "    ssm_conv_bias=config.MODEL.VSSM.SSM_CONV_BIAS,\n",
    "    ssm_drop_rate=config.MODEL.VSSM.SSM_DROP_RATE,\n",
    "    ssm_init=config.MODEL.VSSM.SSM_INIT,\n",
    "    forward_type=config.MODEL.VSSM.SSM_FORWARDTYPE,\n",
    "    # ===================\n",
    "    mlp_ratio=config.MODEL.VSSM.MLP_RATIO,\n",
    "    mlp_act_layer=config.MODEL.VSSM.MLP_ACT_LAYER,\n",
    "    mlp_drop_rate=config.MODEL.VSSM.MLP_DROP_RATE,\n",
    "    # ===================\n",
    "    drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "    patch_norm=config.MODEL.VSSM.PATCH_NORM,\n",
    "    norm_layer=config.MODEL.VSSM.NORM_LAYER,\n",
    "    downsample_version=config.MODEL.VSSM.DOWNSAMPLE,\n",
    "    patchembed_version=config.MODEL.VSSM.PATCHEMBED,\n",
    "    gmlp=config.MODEL.VSSM.GMLP,\n",
    "    use_checkpoint=config.TRAIN.USE_CHECKPOINT,\n",
    "    ) \n",
    "\n",
    "deep_model = deep_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba4448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STMambaSCD(\n",
       "  (encoder): Backbone_VSSM(\n",
       "    (patch_embed): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): Permute()\n",
       "      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Permute()\n",
       "      (4): GELU(approximate='none')\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (6): Permute()\n",
       "      (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (blocks): Sequential(\n",
       "          (0): VSSBlock(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=128, out_features=256, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.0)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): VSSBlock(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=128, out_features=256, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.030000001192092896)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Permute()\n",
       "          (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): Permute()\n",
       "          (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (blocks): Sequential(\n",
       "          (0): VSSBlock(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.06000000238418579)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): VSSBlock(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.09000000357627869)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Permute()\n",
       "          (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): Permute()\n",
       "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (blocks): Sequential(\n",
       "          (0): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.12000000476837158)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.15000000596046448)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.18000000715255737)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.21000000834465027)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.24000000953674316)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.27000001072883606)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.30000001192092896)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.33000001311302185)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.36000001430511475)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.39000001549720764)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.42000001668930054)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.45000001788139343)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.48000001907348633)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.5099999904632568)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): VSSBlock(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.5400000214576721)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Permute()\n",
       "          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): Permute()\n",
       "          (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (blocks): Sequential(\n",
       "          (0): VSSBlock(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "              (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.5700000524520874)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): VSSBlock(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (op): SS2D(\n",
       "              (out_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (in_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (act): SiLU()\n",
       "              (conv2d): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "              (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (dropout): Identity()\n",
       "            )\n",
       "            (drop_path): timm.DropPath(0.6000000238418579)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Identity()\n",
       "      )\n",
       "    )\n",
       "    (outnorm0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (outnorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (outnorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (outnorm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder_bcd): ChangeDecoder(\n",
       "    (st_block_41): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_31): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_21): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_11): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (fuse_layer_1): FFT_Fusion(\n",
       "      (fft_pre): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (fft_post): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (reduce_conv): Conv2d(5120, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (reduce_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reduce_relu): ReLU(inplace=True)\n",
       "      (ch_gate): ChannelGate(\n",
       "        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sp_gate): SpatialGate(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse_layer_2): FFT_Fusion(\n",
       "      (fft_pre): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (fft_post): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (reduce_conv): Conv2d(2560, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (reduce_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reduce_relu): ReLU(inplace=True)\n",
       "      (ch_gate): ChannelGate(\n",
       "        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sp_gate): SpatialGate(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse_layer_3): FFT_Fusion(\n",
       "      (fft_pre): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (fft_post): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (reduce_conv): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (reduce_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reduce_relu): ReLU(inplace=True)\n",
       "      (ch_gate): ChannelGate(\n",
       "        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sp_gate): SpatialGate(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse_layer_4): FFT_Fusion(\n",
       "      (fft_pre): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (fft_post): FFTBranch(\n",
       "        (proj): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (reduce_conv): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (reduce_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reduce_relu): ReLU(inplace=True)\n",
       "      (ch_gate): ChannelGate(\n",
       "        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sp_gate): SpatialGate(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (down_sample_1): PyramidFusion(\n",
       "      (shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1024)\n",
       "        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=96, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=96, out_features=1536, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (down_sample_2): PyramidFusion(\n",
       "      (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=48, out_features=768, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (down_sample_3): PyramidFusion(\n",
       "      (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
       "        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=24, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=24, out_features=384, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_3): ResBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=32, out_features=512, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_2): ResBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_1): ResBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=8, out_features=128, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_T1): SemanticDecoder(\n",
       "    (st_block_4_semantic): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_3_semantic): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_2_semantic): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_1_semantic): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (down_sample_1): PyramidFusion(\n",
       "      (shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1024)\n",
       "        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=96, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=96, out_features=1536, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (down_sample_2): PyramidFusion(\n",
       "      (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=48, out_features=768, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (down_sample_3): PyramidFusion(\n",
       "      (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
       "        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=24, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=24, out_features=384, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_3_semantic): ResBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=32, out_features=512, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_2_semantic): ResBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_1_semantic): ResBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=8, out_features=128, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_0_semantic): ResBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=8, out_features=128, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_T2): SemanticDecoder(\n",
       "    (st_block_4_semantic): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_3_semantic): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_2_semantic): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (st_block_1_semantic): Sequential(\n",
       "      (0): Permute()\n",
       "      (1): VSSBlock(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): SS2D(\n",
       "          (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (in_proj): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (act): SiLU()\n",
       "          (conv2d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "          (dropout): Identity()\n",
       "        )\n",
       "        (drop_path): timm.DropPath(0.1)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Permute()\n",
       "    )\n",
       "    (down_sample_1): PyramidFusion(\n",
       "      (shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1024)\n",
       "        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=96, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=96, out_features=1536, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (down_sample_2): PyramidFusion(\n",
       "      (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=48, out_features=768, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (down_sample_3): PyramidFusion(\n",
       "      (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch3): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (branch5): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
       "        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (channel_att): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=24, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=24, out_features=384, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (spatial_att): SpatialAttention(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_3_semantic): ResBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=32, out_features=512, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_2_semantic): ResBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_1_semantic): ResBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=8, out_features=128, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (smooth_layer_0_semantic): ResBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SqueezeExcitation(\n",
       "        (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "        (excitation): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=8, out_features=128, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (main_clf_cd): PyramidFusion(\n",
       "    (shortcut): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch1): Sequential(\n",
       "      (0): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (branch5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)\n",
       "      (1): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (channel_att): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=6, out_features=0, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=0, out_features=6, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (spatial_att): SpatialAttention(\n",
       "      (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (final_conv): Sequential(\n",
       "      (0): Conv2d(6, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (aux_clf): PyramidFusion(\n",
       "    (shortcut): Conv2d(128, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch1): Sequential(\n",
       "      (0): Conv2d(128, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): Conv2d(128, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (branch5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)\n",
       "      (1): Conv2d(128, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (channel_att): ChannelAttention(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=15, out_features=0, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=0, out_features=15, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (spatial_att): SpatialAttention(\n",
       "      (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (final_conv): Sequential(\n",
       "      (0): Conv2d(15, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.load_state_dict(torch.load(args.resume))\n",
    "\n",
    "\n",
    "deep_model.cuda()\n",
    "deep_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0542c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "MACs: 263.157G\n",
      "Parameters: 189.545M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile, clever_format\n",
    "\n",
    "# Create dummy input matching your model's input shape\n",
    "dummy_pre = torch.randn(1, 3, 512, 512).cuda()\n",
    "dummy_post = torch.randn(1, 3, 512, 512).cuda()\n",
    "\n",
    "macs, params = profile(deep_model, inputs=(dummy_pre, dummy_post))\n",
    "macs, params = clever_format([macs, params], \"%.3f\")\n",
    "\n",
    "print(f\"MACs: {macs}\")\n",
    "print(f\"Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_loader(path):\n",
    "    img = np.array(imageio.imread(path), np.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f35a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_transition_matrix(t1_labels, t2_labels,\n",
    "                           class_names, class_ids,\n",
    "                           title=\"\",\n",
    "                           save_path=None,\n",
    "                           cmap=\"Blues\",\n",
    "                           data_override=None):\n",
    "\n",
    "    if data_override is None:\n",
    "        id_to_idx = {cid: idx for idx, cid in enumerate(class_ids)}\n",
    "        n_cls     = len(class_ids)\n",
    "        trans_mat = np.zeros((n_cls, n_cls), dtype=np.int64)\n",
    "\n",
    "        t1_flat, t2_flat = t1_labels.ravel(), t2_labels.ravel()\n",
    "        mask             = t1_flat != t2_flat\n",
    "        for f, t in zip(t1_flat[mask], t2_flat[mask]):\n",
    "            if f in id_to_idx and t in id_to_idx:\n",
    "                trans_mat[id_to_idx[f], id_to_idx[t]] += 1\n",
    "    else:\n",
    "        trans_mat = data_override\n",
    "        n_cls     = trans_mat.shape[0]\n",
    "\n",
    "    tot   = trans_mat.sum()\n",
    "    pct   = np.where(tot == 0, 0.0, trans_mat / tot * 100)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(pct, cmap=cmap, vmin=0, vmax=pct.max())\n",
    "\n",
    "    ax.set_xticks(range(n_cls)); ax.set_yticks(range(n_cls))\n",
    "    ax.set_xticklabels(class_names, rotation=25)\n",
    "    ax.set_yticklabels(class_names)\n",
    "\n",
    "    ax.xaxis.set_label_position(\"top\"); ax.xaxis.tick_top()\n",
    "    ax.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax.set_xlabel(\"To\"); ax.set_ylabel(\"From\"); ax.set_title(title, pad=10)\n",
    "\n",
    "    for i in range(n_cls):\n",
    "        for j in range(n_cls):\n",
    "            ax.text(j, i, f\"{pct[i,j]:.2f}%\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if pct[i,j] > pct.max()*0.6 else \"black\",\n",
    "                    fontsize=8)\n",
    "\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compute_dataset_transition_matrices(test_imgs,\n",
    "                                        deep_model,\n",
    "                                        class_ids   = [1, 2, 3, 4, 5, 6],\n",
    "                                        class_names = ['low vegetation',\n",
    "                                                       'ground',\n",
    "                                                       'tree',\n",
    "                                                       'water',\n",
    "                                                       'Building',\n",
    "                                                       'Playground'],\n",
    "                                        device      = \"cuda\",\n",
    "                                        save_dir    = \".\"):\n",
    "    \n",
    "\n",
    "    Dataset_Path = LandSat_DATASET_PATH\n",
    "    test_imgs_A = [os.path.join(Dataset_Path, 'A', img) for img in test_imgs]\n",
    "    test_imgs_B = [os.path.join(Dataset_Path, 'B', img) for img in test_imgs]\n",
    "    test_imgs_label_A = [os.path.join(Dataset_Path, 'labelA', img) for img in test_imgs]\n",
    "    test_imgs_label_B = [os.path.join(Dataset_Path, 'labelB', img) for img in test_imgs]\n",
    "\n",
    "    id_to_idx  = {cid: idx for idx, cid in enumerate(class_ids)}\n",
    "    n_cls      = len(class_ids)\n",
    "\n",
    "    gt_counts   = np.zeros((n_cls, n_cls), dtype=np.int64)\n",
    "    pred_counts = np.zeros((n_cls, n_cls), dtype=np.int64)\n",
    "\n",
    "    def add_counts(mat: np.ndarray, a: np.ndarray, b: np.ndarray):\n",
    "        flat_a, flat_b = a.ravel(), b.ravel()\n",
    "        for f, t in zip(flat_a, flat_b):\n",
    "            if f in id_to_idx and t in id_to_idx:\n",
    "                mat[id_to_idx[f], id_to_idx[t]] += 1\n",
    "\n",
    "    deep_model.eval().to(device)\n",
    "\n",
    "    for img_A_path, img_B_path, label_A_path, label_B_path in tqdm(\n",
    "            zip(test_imgs_A, test_imgs_B, test_imgs_label_A, test_imgs_label_B), total=len(test_imgs_A)):\n",
    "        name = os.path.basename(img_A_path)\n",
    "\n",
    "        gt_t1 = imageio.imread(label_A_path)\n",
    "        gt_t2 = imageio.imread(label_B_path)\n",
    "        add_counts(gt_counts, gt_t1, gt_t2)\n",
    "\n",
    "        pre_image  = img_loader(img_A_path)\n",
    "        post_image = img_loader(img_B_path)\n",
    "\n",
    "        pre_tensor = TF.to_tensor(imutils.normalize_img(pre_image)).unsqueeze(0).cuda()\n",
    "        post_tensor = TF.to_tensor(imutils.normalize_img(post_image)).unsqueeze(0).cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_cd, output_t1, output_t2 = deep_model(pre_tensor, post_tensor)\n",
    "\n",
    "            change_mask = torch.argmax(output_cd, axis=1)\n",
    "\n",
    "            preds_A = torch.argmax(output_t1, dim = 1)\n",
    "            preds_B = torch.argmax(output_t2, dim = 1)\n",
    "\n",
    "            preds_A = (preds_A*change_mask.squeeze().long()).cpu().numpy()\n",
    "            preds_B = (preds_B*change_mask.squeeze().long()).cpu().numpy()\n",
    "            \n",
    "\n",
    "        add_counts(pred_counts, preds_A, preds_B)\n",
    "\n",
    "    plot_transition_matrix(None, None,\n",
    "                           class_names=class_names,\n",
    "                           class_ids=class_ids,\n",
    "                        #    title=\"GT  transitions  whole test set\",\n",
    "                           save_path=Path(save_dir) / \"GT_transition_matrix_landsat.png\",\n",
    "                           data_override=gt_counts)\n",
    "\n",
    "    plot_transition_matrix(None, None,\n",
    "                           class_names=class_names,\n",
    "                           class_ids=class_ids,\n",
    "                        #    title=\"Predicted transitions  whole test set\",\n",
    "                           save_path=Path(save_dir) / \"Pred_transition_matrix_landsat.png\",\n",
    "                           data_override=pred_counts)\n",
    "\n",
    "    diff_counts = pred_counts - gt_counts\n",
    "\n",
    "    plot_transition_matrix(None, None,\n",
    "                           class_names=class_names,\n",
    "                           class_ids=class_ids,\n",
    "                        #    title=\"Difference (Pred - GT) transitions  whole test set\",\n",
    "                           save_path=Path(save_dir) / \"Diff_transition_matrix_landsat.png\",\n",
    "                           data_override=diff_counts)   \n",
    "\n",
    "    return gt_counts, pred_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b463e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 477/477 [02:34<00:00,  3.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[      0, 1365749,  664320,       0],\n",
       "        [9330980,       0,  573561, 1787659],\n",
       "        [ 257248,   79125,       0,       0],\n",
       "        [  67628,  828049,       0,       0]]),\n",
       " array([[  19353, 1250450,  660065,     300],\n",
       "        [9444702,   20337,  574745, 1793702],\n",
       "        [ 256970,   76086,    4908,       3],\n",
       "        [  68741,  822982,     410,    1454]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_images = test_data_list\n",
    "\n",
    "compute_dataset_transition_matrices(test_imgs=full_images, deep_model = deep_model, class_ids=[1, 2, 3, 4],class_names=[\n",
    "    'Farmland', 'Desert', 'Building', 'Water'\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0206d8",
   "metadata": {},
   "source": [
    "# Annotations for the LandSat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ceed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_label_value_dict = {\n",
    "    'No change': (255, 255, 255),\n",
    "    'Farmland': (0, 155, 0),\n",
    "    'Desert': (255, 165, 0),\n",
    "    'Building': (230, 30, 100),\n",
    "    'Water': (0, 170, 240)\n",
    "}\n",
    "\n",
    "target_label_value_dict = {\n",
    "    'No change': 0,\n",
    "    'Farmland': 1,\n",
    "    'Desert': 2,\n",
    "    'Building': 3,\n",
    "    'Water': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image_LANDSAT(image_number):\n",
    "    pre_image_path = os.path.join(LandSat_DATASET_PATH,\"A\",f\"{image_number}\")\n",
    "    post_image_path = os.path.join(LandSat_DATASET_PATH,\"B\",f\"{image_number}\")\n",
    "    GT_T1_image_path = os.path.join(LandSat_DATASET_PATH,\"labelA_rgb\",f\"{image_number}\")\n",
    "    GT_T2_image_path = os.path.join(LandSat_DATASET_PATH,\"labelB_rgb\",f\"{image_number}\")\n",
    "\n",
    "    image_number = image_number[:-4] \n",
    "\n",
    "    pre_image = img_loader(pre_image_path)\n",
    "    post_image = img_loader(post_image_path)\n",
    "    GT_T1_image = img_loader(GT_T1_image_path)\n",
    "    GT_T2_image = img_loader(GT_T2_image_path)\n",
    "    \n",
    "    pre_img = imutils.normalize_img(pre_image)  # imagenet normalization\n",
    "    pre_img = np.transpose(pre_img, (2, 0, 1))\n",
    "\n",
    "    post_img = imutils.normalize_img(post_image)  # imagenet normalization\n",
    "    post_img = np.transpose(post_img, (2, 0, 1))\n",
    "\n",
    "    GT_T1_image = np.asarray(GT_T1_image)\n",
    "    GT_T2_image = np.asarray(GT_T2_image)\n",
    "\n",
    "    # save original images\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_pre.png\"), pre_image.astype(np.uint8))\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_post.png\"), post_image.astype(np.uint8))\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_gt_t1.png\"), GT_T1_image.astype(np.uint8))\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_gt_t2.png\"), GT_T2_image.astype(np.uint8))\n",
    "\n",
    "    pre_tensor = TF.to_tensor(imutils.normalize_img(pre_image)).unsqueeze(0).cuda()\n",
    "    post_tensor = TF.to_tensor(imutils.normalize_img(post_image)).unsqueeze(0).cuda()\n",
    "    label_t1 = TF.to_tensor(GT_T1_image).cuda().long()\n",
    "    label_t2 = TF.to_tensor(GT_T2_image).cuda().long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_cd, output_t1, output_t2 = deep_model(pre_tensor, post_tensor)\n",
    "\n",
    "        change_mask = torch.argmax(output_cd, axis=1)\n",
    "\n",
    "        preds_A = torch.argmax(output_t1, dim = 1)\n",
    "        preds_B = torch.argmax(output_t2, dim = 1)\n",
    "\n",
    "        preds_A = (preds_A*change_mask.squeeze().long()).cpu().numpy()\n",
    "        preds_B = (preds_B*change_mask.squeeze().long()).cpu().numpy()\n",
    "\n",
    "    preds_A = map_labels_to_colors(np.squeeze(preds_A), ori_label_value_dict=ori_label_value_dict, target_label_value_dict=target_label_value_dict)\n",
    "    preds_B = map_labels_to_colors(np.squeeze(preds_B), ori_label_value_dict=ori_label_value_dict, target_label_value_dict=target_label_value_dict)\n",
    "\n",
    "\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_pred_t1.png\"), preds_A.astype(np.uint8))\n",
    "    imageio.imwrite(os.path.join(SAVE_IMGS_PATH, f\"{image_number}_pred_t2.png\"), preds_B.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45ada3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate_image_LANDSAT(\"From1997To2017_12.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
